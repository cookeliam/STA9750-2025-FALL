---
title: "Predicting Neighborhood Income from Housing Complaint Patterns in New York City"
author: Liam Cooke
date: "18 December 2025"
format: 
  html:
    code-fold: true
    toc: true
    toc-depth: 3
    self-contained: true
editor: visual
---

## 1. Introduction and Overarching Question

New York City’s housing complaint systems generate a detailed administrative record of how residents interact with housing and municipal services. These data are widely used to study housing quality, neighborhood distress, and public service delivery. However, complaint records reflect not only underlying housing conditions, but also reporting behavior, access to services, and administrative processes. Understanding how these systems relate to neighborhood socioeconomic context is therefore essential for interpreting complaint patterns.

With this, the team's overarching question motivating this course project is: 

> **To what extent do neighborhood income levels predict the intensity, composition, and resolution of housing-related complaints in New York City, after controlling for structural housing characteristics?**

This question reflects a broader interest in understanding how socioeconomic conditions shape interactions between residents and municipal service systems. Housing complaints provide a rich source of administrative data that may reveal disparities in housing quality, reporting behavior, and service responsiveness across neighborhoods.

This individual report contributes to the overarching question by focusing on modeling and interpretation. Rather than testing a single causal hypothesis, my contribution is to **build and evaluate a predictive model of neighborhood median income using engineered features derived from the HPD, DOB, and 311 datasets and identify which features are most influential for predicting income**. If income is highly predictable from these features, then income is not merely an explanatory variable applied after the fact; It is deeply embedded in the structure of the complaint record itself.

To answer this question, I construct tract-level predictors capturing complaint intensity (complaints per x units), composition (shares of complaints by source and type), and outcomes (typical resolution times). I compare linear models (OLS and Elastic Net) to a non-linear Random Forest using cross-validation and a held-out test set, and apply permutation-based methods to identify which features meaningfully contribute to predictive performance. The goal is not causal inference, but rather to assess the extent to which service-system data reflect neighborhood socioeconomic context, thereby informing how the team’s main OQ should be interpreted.

```{r, echo=FALSE}
#| output: false
# Data + APIs
library(devtools)
library(RSocrata)
library(janitor)
library(lubridate)
library(dplyr)
library(tidycensus)
library(tidyverse)

# Spatial
library(sf)
library(tigris)
options(tigris_use_cache = TRUE)

# Modeling / viz
library(ggplot2)
library(ranger)
library(pdp)
library(tmap)
library(effects)
library(stringr)
library(tidygeocoder)
library(caret)
library(glmnet)
library(patchwork)
library(scales)
library(tibble)
library(purrr)
library(DT)
library(tidyr)

# Inference / hypothesis testing
library(lmtest)
library(sandwich)
library(car)

FULL_PAGINATION <- TRUE

# API Keys
app_token <- "ExUt22WiPiJvXNo3ZwPuIUDuC"
census_api_key("5cd8f72bd473b6e815af190efa42dfdcd3804b99", install = FALSE)

nyc_counties <- c("New York", "Kings", "Queens", "Bronx", "Richmond")

style_dt <- function(df, caption) {
  df |>
    datatable(
      options = list(dom = 't', paging = FALSE, searching = FALSE),
      caption = caption
    )
}
```

---

## 2. Data Sources and Feature Construction

This analysis integrates multiple publicly available datasets, all aggregated to the census tract level to enable consistent spatial analysis.

The primary data sources include:

- **NYC 311 Service Requests**, accessed via the NYC Open Data API, providing detailed records on complaint types, dates, and resolution outcomes.
- **Housing Preservation and Development (HPD)** complaint and violation data, capturing housing code enforcement activity.
- **Department of Buildings (DOB)** complaint data, reflecting structural and construction-related issues.
- **American Community Survey (ACS)** 5-year estimates, accessed via the `tidycensus` API, supplying demographic and housing characteristics such as median income, population, and housing stock.

Because NYC Open Data APIs impose record limits per request, I implement a reusable pagination helper that iteratively downloads data in batches until all 2023 records are retrieved. This approach avoids manual downloads and ensures the report can be re-rendered without local files. Each dataset is cleaned using consistent variable naming and filtered to 2023 records.

```{r}
#| code-summary: "Pagination Helper Function"

# Helper function for pagination/navigation of open data api
read_socrata_2023 <- function(
  base_url,
  date_col,
  limit = 100000,
  app_token = NULL,
  date_format = c("timestamp", "mdy"),
  full = TRUE  # FALSE = testing mode
) {

  date_format <- match.arg(date_format)
  offset <- 0L
  limit  <- as.integer(limit)
  out <- list()

  # Dataset-specific where clause since 311,hpd,dob have different date formats
  where <- if (date_format == "timestamp") {
    sprintf(
      "%s >= '2023-01-01T00:00:00.000' AND %s < '2024-01-01T00:00:00.000'",
      date_col, date_col
    )
  } else {
    sprintf(
      "%s >= '01/01/2023' AND %s < '01/01/2024'",
      date_col, date_col
    )
  }

  repeat {
    offset_str <- format(as.integer(offset), scientific = FALSE)
    url <- paste0(
      base_url,
      "?$where=", URLencode(where, reserved = TRUE),
      "&$limit=", limit,
      "&$offset=", offset_str
    )

    chunk <- read.socrata(url, app_token = app_token) %>%
      clean_names()

    if (nrow(chunk) == 0) break

    out[[length(out) + 1]] <- chunk

    # Exit early in testing mode after first successful chunk
    if (!full) break

    # Stop if the last chunk was partial (end of data)
    if (nrow(chunk) < limit) break

    offset <- offset + limit
  }

  dplyr::bind_rows(out)
}
```

```{r}
#| output: false
#| code-summary: "Raw Dataset Acquisition"
url_hpd <- "https://data.cityofnewyork.us/resource/wvxf-dwi5.csv"
hpd_raw <- read_socrata_2023(
 base_url   = url_hpd,
 date_col   = "novissueddate",
 limit      = 100000,
 app_token  = app_token,
 date_format = "timestamp",
 full = FULL_PAGINATION
)

url_dob <- "https://data.cityofnewyork.us/resource/vztk-gaf7.csv"
dob_raw <- read_socrata_2023(
 base_url   = url_dob,
 date_col   = "date_entered",
 limit      = 100000,
 app_token  = app_token,
 date_format = "mdy",
 full = FULL_PAGINATION
)
 
url_311 <- "https://data.cityofnewyork.us/resource/erm2-nwe9.json"
nyc311_raw <- read_socrata_2023(
 base_url   = url_311,
 date_col   = "created_date",
 limit      = 100000,
 app_token  = app_token,
 date_format = "timestamp",
 full = FULL_PAGINATION
) %>%
 clean_names()

# ACS (panel 2013–2023, tract-level, no geometry)
vars_panel <- c(
  income        = "B19013_001E",
  rent_burden   = "B25070_001E",
  housing_units = "B25002_001E",
  population    = "B01003_001E"
)
years <- 2013:2023

acs_list <- lapply(years, function(y) {
  get_acs(
    geography = "tract",
    variables = vars_panel,
    year      = y,
    survey    = "acs5",
    state     = "NY",
    county    = nyc_counties,
    output    = "wide"
  ) %>%
    mutate(year = y)
})

acs_nyc_tracts <- bind_rows(acs_list)

# ACS with geometry (2023 only, for maps/spatial joins)
vars_geo <- c(
  income        = "B19013_001E",
  rent_burden   = "B25070_001E",
  housing_units = "B25002_001E",
  population    = "B01003_001E"
)

acs_tracts_sf <- get_acs(
  geography = "tract",
  variables = vars_geo,
  year      = 2023,
  survey    = "acs5",
  state     = "NY",
  county    = nyc_counties,
  output    = "wide",
  geometry  = TRUE
) %>%
  st_transform(4326)
```

```{r}
#| code-summary: "Raw Dataset Cleaning"
#| output: false
# HPD Cleaning (2023, classes A/B/C)
hpd_clean <- hpd_raw %>%
 mutate(
   novissueddate           = ymd(novissueddate),
   inspectiondate          = ymd(inspectiondate),
   approveddate            = ymd(approveddate),
   originalcertifybydate   = ymd(originalcertifybydate),
   originalcorrectbydate   = ymd(originalcorrectbydate),
   newcertifybydate        = ymd(newcertifybydate),
   newcorrectbydate        = ymd(newcorrectbydate),
   certifieddate           = ymd(certifieddate),
   currentstatusdate       = ymd(currentstatusdate),
   date                    = novissueddate,
   year                    = year(date),
   month                   = month(date),
   dow                     = wday(date, label = TRUE),
 ) %>%
 filter(
   between(date, as.Date("2023-01-01"), as.Date("2023-12-31")),
   class %in% c("A", "B", "C"),
   !is.na(boro)
 )

# 311 Cleaning (2023, keep geocoded)
nyc311_clean <- nyc311_raw %>%
 mutate(
   created_date = ymd_hms(created_date),
   closed_date  = ymd_hms(closed_date),
   date         = as_date(created_date),
   year         = year(created_date),
   month        = month(created_date),
   dow          = wday(created_date, label = TRUE)
 ) %>%
 filter(
   between(date, as.Date("2023-01-01"), as.Date("2023-12-31")),
   !is.na(latitude),
   !is.na(longitude)
 )

# DOB Cleaning (2023)
dob_clean <- dob_raw %>%
 mutate(
   date_entered     = mdy(date_entered),
   inspection_date  = mdy(inspection_date),
   disposition_date = mdy(disposition_date),
   date  = date_entered,
   year  = year(date_entered),
   month = month(date_entered),
   dow   = wday(date_entered, label = TRUE),
   full_address = paste0(house_number, " ", house_street, ", NY ", zip_code)
 ) %>%
 filter(between(date, as.Date("2023-01-01"), as.Date("2023-12-31")))

# ACS Cleaning (panel)
acs_clean <- acs_nyc_tracts %>%
  mutate(
    tract_geoid   = GEOID,
    median_income = income,
    rent_burden   = rent_burden,
    housing_units = housing_units,
    population    = as.numeric(population)
  ) %>%
  select(tract_geoid, median_income, rent_burden, housing_units, population, year)
```

A key technical challenge is spatial alignment. HPD and 311 records include latitude and longitude, while DOB records require address-based geocoding. I convert all point-based complaint data to sf objects and spatially join them to NYC census tract geometries.

After spatial joins, complaint data are aggregated to the tract level. For each tract, I compute complaint rates per 100/1000 units, complaint shares by source (HPD, DOB, 311), the share of heat-related 311 complaints, and summary statistics of complaint resolution time (mean and median). These engineered features form the core predictors used in the modeling stage.

From these sources, several families of features were constructed:

- **Structural housing controls**, including rent burden, total housing units, population, and borough indicators.
- **Complaint intensity measures**, such as per-unit 311 complaint rates.
- **Complaint composition measures**, including the share of complaints related to specific issues (e.g., heat-related complaints).
- **Resolution measures**, summarizing average and median complaint resolution times.

```{r}
#| code-summary: "Spatial Joins, Tract Aggregation, and Engineered Features"
#| output: false
# Tract polygons (2023, NY State Plane)
tracts_sf <- tracts(
  state  = "NY",
  county = nyc_counties,
  year   = 2023,
  cb     = TRUE
) %>%
  st_transform(2263)

# 311: points -> tract GEOID
nyc311_sf <- nyc311_clean %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, remove = FALSE) %>%
  st_transform(2263)

nyc311_join <- st_join(nyc311_sf, tracts_sf["GEOID"], left = TRUE) %>%
  st_drop_geometry() %>%
  rename(tract_geoid = GEOID)

# HPD: point geometry -> tract GEOID
hpd_sf <- hpd_clean %>%
  filter(!is.na(longitude), !is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, remove = FALSE) %>%
  st_transform(2263)

hpd_join <- st_join(hpd_sf, tracts_sf["GEOID"], left = TRUE) %>%
  st_drop_geometry() %>%
  rename(tract_geoid = GEOID)

hpd_counts <- hpd_join %>%
  group_by(tract_geoid, year) %>%
  summarise(
    hpd_complaints = n(),
    .groups = "drop"
  )

# HPD Class C counts by tract/year
hpd_counts_classc <- hpd_join %>%
  group_by(tract_geoid, year) %>%
  summarise(
    hpd_classc = sum(class == "C", na.rm = TRUE),
    .groups = "drop"
  )

# DOB: geocode -> points -> tract
dob_geo <- dob_clean %>%
  geocode(
    address = full_address,
    method  = "census",
    lat     = latitude,
    long    = longitude
  )

dob_geo_clean <- dob_geo %>%
  filter(!is.na(latitude), !is.na(longitude))

dob_sf <- dob_geo_clean %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, remove = FALSE) %>%
  st_transform(2263)

dob_join <- st_join(dob_sf, tracts_sf["GEOID"], left = TRUE) %>%
  st_drop_geometry() %>%
  rename(tract_geoid = GEOID)

dob_counts <- dob_join %>%
  group_by(tract_geoid, year) %>%
  summarise(
    dob_complaints = n(),
    .groups = "drop"
  )

# 311 counts by tract/year
nyc311_counts <- nyc311_join %>%
  group_by(tract_geoid, year) %>%
  summarise(
    nyc311_complaints = n(),
    .groups = "drop"
  )

# 311 Heat/hot water counts by tract/year
nyc311_heat_counts <- nyc311_join %>%
  group_by(tract_geoid, year) %>%
  summarise(
    nyc311_heat = sum(complaint_type == "HEAT/HOT WATER", na.rm = TRUE),
    .groups = "drop"
  )

# Panel Dataset: Complaints and ACS by tract/year
complaints_by_tract <- acs_clean %>%
  left_join(hpd_counts,         by = c("tract_geoid", "year")) %>%
  left_join(hpd_counts_classc,  by = c("tract_geoid", "year")) %>%
  left_join(dob_counts,         by = c("tract_geoid", "year")) %>%
  left_join(nyc311_counts,      by = c("tract_geoid", "year")) %>%
  left_join(nyc311_heat_counts, by = c("tract_geoid", "year")) %>%
  mutate(
    hpd_complaints    = coalesce(hpd_complaints, 0),
    hpd_classc        = coalesce(hpd_classc, 0),
    dob_complaints    = coalesce(dob_complaints, 0),
    nyc311_complaints = coalesce(nyc311_complaints, 0),
    nyc311_heat       = coalesce(nyc311_heat, 0),

    # per-100-housing-unit complaint intensity
    hpd_rate    = (hpd_complaints    / housing_units) * 100,
    dob_rate    = (dob_complaints    / housing_units) * 100,
    nyc311_rate = (nyc311_complaints / housing_units) * 100,

    # composition / severity proxies
    share_heat_311     = if_else(nyc311_complaints > 0, nyc311_heat / nyc311_complaints, 0),
    share_classc_hpd   = if_else(hpd_complaints > 0, hpd_classc / hpd_complaints, 0)
  )

# HPD unresolved
hpd_q4 <- hpd_join %>%
  mutate(
    violationstatus_clean = str_to_upper(coalesce(violationstatus, "")),
    unresolved_hpd = if_else(
      str_detect(violationstatus_clean, "CLOSE"),
      0L, 1L
    )
  ) %>%
  group_by(tract_geoid, year) %>%
  summarise(
    hpd_n          = n(),
    hpd_unres      = sum(unresolved_hpd, na.rm = TRUE),
    hpd_unres_rate = hpd_unres / hpd_n,
    .groups = "drop"
  )

# DOB unresolved
dob_q4 <- dob_join %>%
  mutate(
    status_clean = str_to_upper(coalesce(status, "")),
    unresolved_dob = if_else(
      str_detect(status_clean, "CLOSED"),
      0L, 1L
    )
  ) %>%
  group_by(tract_geoid, year) %>%
  summarise(
    dob_n          = n(),
    dob_unres      = sum(unresolved_dob, na.rm = TRUE),
    dob_unres_rate = dob_unres / dob_n,
    .groups = "drop"
  )

# Merge into ACS
complaints_q4 <- acs_clean %>%
  left_join(hpd_q4, by = c("tract_geoid", "year")) %>%
  left_join(dob_q4, by = c("tract_geoid","year")) %>%
  mutate(
    total_n = coalesce(hpd_n,0) + coalesce(dob_n,0),
    total_unres =
      coalesce(hpd_unres,0) + coalesce(dob_unres,0),
    total_unres_rate = if_else(
      total_n > 0, total_unres / total_n, 0
    )
  )

# Resolution-time features from 311
tract_resolution <- nyc311_join %>%
  filter(!is.na(created_date), !is.na(closed_date)) %>%
  mutate(
    resolution_days_311 = as.numeric(closed_date - created_date, units = "days")
  ) %>%
  filter(
    resolution_days_311 >= 0,
    resolution_days_311 <= 365
  ) %>%
  group_by(tract_geoid) %>%
  summarise(
    resolution_days_avg    = mean(resolution_days_311, na.rm = TRUE),
    resolution_days_median = median(resolution_days_311, na.rm = TRUE),
    n_cases                = n(),
    .groups = "drop"
  )

# Borough indicator from tract polygons
borough_lookup <- tracts_sf %>%
  st_drop_geometry() %>%
  select(tract_geoid = GEOID, COUNTYFP) %>%
  mutate(
    borough = case_when(
      COUNTYFP == "005" ~ "Bronx",
      COUNTYFP == "047" ~ "Brooklyn",
      COUNTYFP == "061" ~ "Manhattan",
      COUNTYFP == "081" ~ "Queens",
      COUNTYFP == "085" ~ "Staten Island",
      TRUE              ~ NA_character_
    )
  )

# Population lookup from ACS geometry pull (2023)
acs_pop_2023 <- acs_tracts_sf %>%
  st_drop_geometry() %>%
  transmute(
    tract_geoid = GEOID,
    population  = as.numeric(population)
  )
```

## 3. Dataset Engineering and Modeling Strategy
This section describes the construction of the modeling dataset, the rationale for the modeling choices, and the procedures used for model tuning and evaluation.

### 3.1 Modeling Dataset and Target Variable
The final modeling dataset consists of census-tract-level observations for New York City in 2023, with each row corresponding to a single tract. The outcome variable is log-transformed median household income, obtained from the ACS 5-year estimates. The logarithmic transformation reduces right skew and allows model performance to be interpreted in terms of proportional differences in income rather than absolute dollar differences.

Predictors are derived exclusively from administrative housing service records and fall into the feature categories previously mentioned. In addition, borough indicator variables are included to capture broad geographic and administrative differences across New York City that are not directly observable in the complaint data. All predictors are measured at the tract level and aligned temporally to 2023. Observations with missing outcome values or missing core predictors are excluded prior to modeling.

```{r}
#| code-summary: "Model Dataset aggregation and cleaning"

# Combine Q1–Q4 features + borough
income_model_df <- acs_clean %>%
  filter(year == 2023) %>%
  select(tract_geoid, median_income, rent_burden, housing_units) %>%
  left_join(acs_pop_2023, by = "tract_geoid") %>%
  left_join(
    complaints_by_tract %>%
      filter(year == 2023) %>%
      select(tract_geoid, hpd_rate, dob_rate, nyc311_rate, nyc311_complaints, share_heat_311, share_classc_hpd),
    by = "tract_geoid"
  ) %>%
  left_join(
    complaints_q4 %>%
      filter(year == 2023) %>%
      select(tract_geoid, hpd_unres_rate, dob_unres_rate, total_unres_rate),
    by = "tract_geoid"
  ) %>%
  left_join(tract_resolution, by = "tract_geoid") %>%
  left_join(borough_lookup,   by = "tract_geoid") %>%
  mutate(
    population = replace_na(population, median(population, na.rm = TRUE)),
    nyc311_complaints = replace_na(nyc311_complaints, 0),
    nyc311_per_1000_pop = if_else(population > 0, (nyc311_complaints / population) * 1000, 0),
    share_heat_311 = replace_na(share_heat_311, 0),
    share_classc_hpd = replace_na(share_classc_hpd, 0),

    total_rate = hpd_rate + dob_rate + nyc311_rate,
    share_hpd  = if_else(total_rate > 0, hpd_rate  / total_rate, NA_real_),
    share_dob  = if_else(total_rate > 0, dob_rate  / total_rate, NA_real_),
    share_311  = if_else(total_rate > 0, nyc311_rate / total_rate, NA_real_)
  )

# Clean modeling dataset
model_df <- income_model_df %>%
  filter(!is.na(median_income)) %>%
  mutate(
    log_income = log(median_income),
    borough    = factor(borough),
    
    # complaint rates and shares: missing = no activity
    across(
      c(hpd_rate, dob_rate, nyc311_rate, nyc311_per_1000_pop,
        hpd_unres_rate, dob_unres_rate, total_unres_rate,
        share_hpd, share_dob, share_311, share_heat_311, share_classc_hpd),
      ~replace_na(.x, 0)
    ),
    
    # resolution timing: missing = citywide median
    resolution_days_avg = replace_na(
      resolution_days_avg,
      median(resolution_days_avg, na.rm = TRUE)
    ),
    
    resolution_days_median = replace_na(
      resolution_days_median,
      median(resolution_days_median, na.rm = TRUE)
    ),
    
    # ACS rent burden: median fallback
    rent_burden = replace_na(
      rent_burden,
      median(rent_burden, na.rm = TRUE)
    ),

    # population: median fallback
    population = replace_na(
      population,
      median(population, na.rm = TRUE)
    )
  ) %>%
  select(
    tract_geoid,
    log_income,
    median_income,
    rent_burden,
    housing_units,
    population,
    hpd_rate,
    dob_rate,
    nyc311_rate,
    nyc311_per_1000_pop,
    hpd_unres_rate,
    dob_unres_rate,
    total_unres_rate,
    resolution_days_avg,
    resolution_days_median,
    share_hpd,
    share_dob,
    share_311,
    share_heat_311,
    share_classc_hpd,
    borough
  )
```


### 3.2 Train-Test Split and Cross-Validation Framework
To obtain an unbiased assessment of predictive performance, the dataset is partitioned into a training set (70%) and a held-out test set (30%) using random sampling. Hyperparameters are selected using 5-fold cross-validation on the full tract-level dataset, and final performance is evaluated on a separate held-out test split.

Within the training set, I use 5-fold cross-validation to estimate model performance and tune hyperparameters. Five-fold CV represents a balance between computational efficiency and variance reduction given the tract-level sample size. Model performance during cross-validation is evaluated using root mean squared error (RMSE) as the primary selection criterion, with mean absolute error (MAE) and out-of-sample R^2 reported for interpretability and comparison across model classes.


### 3.3 Baseline Model Comparison: OLS, Elastic Net, and Random Forest
I begin with two linear baselines. First, Ordinary Least Squares (OLS) serves as a transparent benchmark that assumes linear relationships between predictors and log income. While unlikely to capture complex interactions among complaint features, OLS provides a useful reference point for assessing whether more flexible models meaningfully improve predictive performance.

Second, I estimate an Elastic Net regression using the `glmnet` framework. Elastic Net combines L1 (lasso) and L2 (ridge) penalties, allowing for coefficient shrinkage and partial variable selection while retaining stability in the presence of correlated predictors. Elastic Net provides an interpretable compromise between unrestricted OLS and more flexible non-parametric methods, and it is particularly well suited to settings with correlated predictors such as complaint shares and rates.

```{r}
#| code-summary: "OLS and Elastic Net Testing"
#| output: false
# Cross-validation setup
set.seed(123)

# Model features
rf_formula <- log_income ~ rent_burden + housing_units + population +
  hpd_rate + dob_rate +
  nyc311_per_1000_pop +
  resolution_days_avg + resolution_days_median +
  share_hpd + share_dob + share_311 +
  share_heat_311 + share_classc_hpd +
  borough

cv_df <- model_df %>%
  select(all.vars(rf_formula), median_income, tract_geoid) %>%
  drop_na()

cv_control <- trainControl(
  method = "cv",
  number = 5,
  savePredictions = "final"
)

# OLS baseline 
set.seed(123)
cv_lm <- train(
  rf_formula,
  data = cv_df,
  method = "lm",
  trControl = cv_control,
  metric = "RMSE"
)

# Elastic Net (glmnet)
set.seed(123)
cv_glmnet <- train(
  rf_formula,
  data = cv_df,
  method = "glmnet",
  trControl = cv_control,
  tuneLength = 25,
  metric = "RMSE"
)
```

To capture potential non-linearities and interactions among complaint features, I estimate a Random Forest model using the ranger implementation. Random Forests are well suited to this application because they require minimal parametric assumptions, are robust to multicollinearity, and can naturally model complex interaction effects.

Three key hyperparameters are tuned via cross-validation:

- `mtry`: the number of predictors randomly considered at each split
- `splitrule`: the splitting criterion (variance vs. extratrees)
- `min.node.size`: the minimum number of observations in a terminal node

The table of the top 10 results (with the best chosen parameters highlighted) for this test are in the code/table below:

```{r}
#| code-summary: "Random Forest Testing and Tuning"
# Random Forest (ranger)
set.seed(123)

rf_grid <- expand.grid(
  mtry = c(4, 6, 8, 10, 12, 14),
  splitrule = c("variance", "extratrees"),
  min.node.size = c(5, 10)
)

#Randmo forest parameter tuning
cv_rf <- train(
  rf_formula,
  data = cv_df,
  method = "ranger",
  trControl = cv_control,
  tuneGrid = rf_grid,
  num.trees = 1000,
  importance = "impurity",
  respect.unordered.factors = "order",
  metric = "RMSE"
)

#Combine all model results
results <- resamples(
  list(
    OLS   = cv_lm,
    ENET  = cv_glmnet,
    RF    = cv_rf
  )
)

# Extract CV results
rf_cv_tbl <- cv_rf$results %>%
  arrange(RMSE) %>%
  transmute(mtry, splitrule, min.node.size, RMSE, Rsquared, MAE) %>%
  slice_head(n = 10)

# I use JS here to help highlight the top row only
best_row <- 1  # R index (1 = first row in rf_cv_tbl)
best_row_js <- best_row - 1  # JS index (0-based)

datatable(
  rf_cv_tbl,
  options = list(
    dom = "t",
    paging = FALSE,
    searching = FALSE,
    rowCallback = JS(sprintf(
      "function(row, data, index) {
         if (index === %d) {
           $('td', row).css({'background-color': '#e6f2ff', 'font-weight': 'bold'});
         }
       }",
      best_row_js
    ))
  ),
  caption = "Random Forest Cross-Validation Results Across Hyperparameter Grid"
) %>%
  formatRound(columns = c("RMSE", "Rsquared", "MAE"), digits = 3)
```

I evaluate a grid of plausible values for each parameter using 5-fold cross-validation on the training set, selecting the combination that minimizes cross-validated RMSE. The final model uses 1,000 trees to ensure stable predictions and importance estimates.

This explicit tuning process is essential, as rather than relying on default Random Forest settings, hyperparameters are chosen to optimize data outside of the sample while controlling model complexity.

### 3.4 Model Comparison and Selection
After tuning, I compare OLS, Elastic Net, and Random Forest models using consistent cross-validation folds. Performance is summarized using RMSE, MAE, and R^2 across folds, allowing direct comparison of both accuracy and stability:

```{r}
#| code-summary: "RMSE Visualization"

rmse_long <- results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = c("Model", "Metric"),
    names_sep = "~",
    values_to = "Value"
  ) %>%
  filter(Metric == "RMSE") %>%
  mutate(Model = factor(Model, levels = c("RF", "ENET", "OLS")))

ggplot(rmse_long, aes(x = Value, y = Model)) +
  geom_boxplot(
    width = 0.55,
    fill = "#302B2B",
    color = "#302B2B",
    alpha = 0.25,
    outlier.shape = NA
  ) +
  geom_point(
    color = "#302B2B",
    size = 2.4,
    alpha = 0.9
  ) +
  labs(
    title = "Cross-Validated Model Performance (RMSE)",
    subtitle = "Boxes show distribution across folds; points indicate individual fold results",
    x = "Root Mean Squared Error (RMSE)",
    y = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 11),
    axis.title.x = element_text(size = 11),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30")
  )
```

```{r}
#| code-summary: "R^2 Visualization"

r2_long <- results$values %>%
    pivot_longer(
        cols = -Resample,
        names_to = c("Model", "Metric"),
        names_sep = "~",
        values_to = "Value"
    ) %>%
    filter(Metric == "Rsquared") %>%
    mutate(Model = factor(Model, levels = c("RF", "ENET", "OLS")))

ggplot(r2_long, aes(x = Value, y = Model)) +
    geom_boxplot(
        width = 0.55,
        fill = "#302B2B",
        color = "#302B2B",
        alpha = 0.25,
        outlier.shape = NA
    ) +
    geom_point(
        color = "#302B2B",
        size = 2.4,
        alpha = 0.9
    ) +
    labs(
        title = ("Cross-Validated Model Performance (R^2)"),
        subtitle = ("Boxes show distribution across folds; points indicate individual fold results"),
        x = ("Coefficient of Determination R^2"),
        y = NULL
    ) +
    theme_minimal(base_size = 14) +
    theme(
        panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(),
        axis.text.y = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.title.x = element_text(size = 11),
        plot.title = element_text(face = "bold"),
        plot.subtitle = element_text(color = "gray30")
    )
```

Across all metrics, the Random Forest demonstrates superior predictive performance relative to the linear models. The improvement in out-of-sample R^2 indicates that nonlinearities and interactions among complaint intensity, composition, and resolution variables contribute meaningfully to explaining variation in neighborhood income. Elastic Net performs similarly to OLS, suggesting that regularization alone is insufficient to capture the underlying structure in the data.

Based on these results, the **Random Forest is selected as the primary predictive model for subsequent interpretation and permutation-based inference**, while linear models are retained for transparency and complementary statistical testing.


### 3.5 Random Forest Implementation and Use of Permutation Methods
The Random Forest is implemented using the ranger package with hyperparameters selected via cross-validation and 1,000 trees to ensure stable predictions. To interpret the fitted model, I rely on permutation-based variable importance rather than impurity-based measures.

Permutation importance is preferred because it directly measures how much each feature contributes to out-of-sample predictive accuracy. In contrast, impurity-based importance is known to be biased toward variables with many split points and can overstate the importance of correlated predictors, which are common in complaint share and rate variables. Permutation importance mitigates these issues by quantifying the drop in model performance when a feature’s values are randomly shuffled, holding all other features fixed.

This approach aligns the interpretation of the Random Forest with the study’s goal of identifying which complaint characteristics meaningfully encode neighborhood income, based on their contribution to predictive performance rather than tree construction artifacts.

```{r}
#| output: false
#| code-summary: "Final Random Forest Model"

# Train / test split
set.seed(123)
n <- nrow(model_df)
train_ids <- sample(seq_len(n), size = floor(0.7 * n))

train_df <- model_df[train_ids, ]
test_df  <- model_df[-train_ids, ]

# Fit Random Forest
rf_income <- ranger(
  rf_formula,
  data = train_df,
  num.trees = 1000,
  mtry = cv_rf$bestTune$mtry,
  splitrule = cv_rf$bestTune$splitrule,
  min.node.size = cv_rf$bestTune$min.node.size,
  importance = "permutation",
  respect.unordered.factors = "order",
  seed = 123
)

# Compute permutation importance from the random forest, build a tidy table of the top 10 predictors
imp_perm <- sort(rf_income$variable.importance, decreasing = TRUE)
imp_tbl <- tibble(
  feature = names(imp_perm),
  perm_importance = as.numeric(imp_perm)
) %>%
  arrange(desc(perm_importance)) %>%
  slice(1:10)
```


## 4. Model Results

Across all performance metrics, the Random Forest consistently outperforms linear alternatives. When evaluated on the held-out test set, the Random Forest attains an R^2 of approximately 0.54, indicating that more than half of the variation in tract-level log income can be explained using complaint and resolution features alone. Predicted versus actual plots show close alignment across most of the income distribution, with modest under-prediction at the upper tail:
```{r}
#| code-summary: "R^2 and RMSE Table"
# Test-set predictions
rf_pred <- predict(rf_income, data = test_df)$predictions

plot_df <- tibble(
  actual = test_df$log_income,
  pred   = rf_pred
)

# Compute performance metrics
rf_test_perf <- tibble(
  Metric = c("R-squared", "RMSE"),
  Value  = c(
    cor(plot_df$actual, plot_df$pred, use = "complete.obs")^2,
    sqrt(mean((plot_df$pred - plot_df$actual)^2, na.rm = TRUE))
  )
)

style_dt(
  rf_test_perf,
  caption = "Random Forest Test-Set Predictive Performance"
) %>%
  formatRound(
    columns = "Value",
    digits = 3
  )
```

```{r, warning=FALSE, message=FALSE}
#| code-summary: "Predicted vs. Observed Visualization"

#Calculate r^2 and rmse
r2   <- cor(plot_df$actual, plot_df$pred, use = "complete.obs")^2
rmse <- sqrt(mean((plot_df$pred - plot_df$actual)^2, na.rm = TRUE))

lab_txt <- sprintf("R^2 = %.3f\nRMSE = %.3f", r2, rmse)

ggplot(plot_df, aes(x = actual, y = pred)) +
  geom_point(
    color = "#302B2B",
    alpha = 0.35,
    size = 1.8
  ) +
  stat_density_2d(
    color = "#302B2B",
    linewidth = 0.4
  ) +
  geom_abline(
    slope = 1,
    intercept = 0,
    linewidth = 1.2,
    color = "gray20"
  ) +
  annotate(
    "label",
    x = -Inf, y = Inf,
    hjust = -0.05, vjust = 1.1,
    label = lab_txt,
    size = 4,
    label.size = 0,
    fill = alpha("white", 0.8)
  ) +
  labs(
    title = "Predicted vs. Observed Neighborhood Income",
    subtitle = "Random forest model, test set",
    x = "Observed log income",
    y = "Predicted log income"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    axis.text = element_text(size = 11),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30")
  )
```

These results suggest that the housing service system encodes a substantial socioeconomic signal. While income is not fully determined by complaint characteristics, complaint data clearly reflect neighborhood-level economic context in systematic ways.

## 5. Feature Importance

To better understand which features drive predictive performance, I examine Random Forest permutation importance. Structural variables such as rent burden and housing stock emerge as the most influential predictors, followed by complaint composition measures and resolution metrics. Borough indicators also rank highly, reflecting persistent geographic heterogeneity:

```{r}
#| code-summary: "Predictors of Income Visualization"

# Add labels to make feature variable names easier to understand
pretty_labels <- c(
  rent_burden            = "Rent burden (% income to rent)",
  borough                = "Borough",
  share_hpd              = "Share of HPD events",
  share_311              = "Share of 311 events",
  hpd_rate               = "HPD rate (per building / unit)",
  resolution_days_avg    = "Avg resolution time (days)",
  housing_units          = "Housing units (ACS)",
  share_heat_311         = "Share of 311: heat/hot water",
  nyc311_per_1000_pop    = "311 requests per 1,000 residents",
  share_classc_hpd       = "HPD Class C (hazardous) Share",
  resolution_days_median = "Median resolution time (days)",
  population             = "Population (ACS)",
  share_dob              = "Share of DOB events",
  dob_rate               = "DOB rate (per building / unit)"
)

imp_tbl <- tibble(
  feature = names(imp_perm),
  perm_importance = as.numeric(imp_perm)
) %>%
  arrange(desc(perm_importance)) %>%
  slice(1:10) %>%
  mutate(
    feature_label = dplyr::recode(feature, !!!pretty_labels, .default = feature),
    feature_label = fct_reorder(feature_label, perm_importance)
  )

ggplot(imp_tbl, aes(x = feature_label, y = perm_importance)) +
  geom_col(
    width = 0.7,
    fill = "#302B2B"
  ) +
  geom_text(
    aes(label = percent(perm_importance, accuracy = 0.1)),
    hjust = -0.08,
    size = 4
  ) +
  coord_flip() +
  scale_y_continuous(
    labels = percent_format(accuracy = 0.1),
    expand = expansion(mult = c(0, 0.18))
  ) +
  labs(
    title = "Most Important Predictors of Income",
    subtitle = "Permutation importance from random forest model",
    x = NULL,
    y = "Increase in prediction error"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30"),
    plot.margin = margin(t = 15, r = 10, b = 10, l = 10)
  )
```

Permutation importance, the metric in the graph above, is measured as the average increase in out-of-sample prediction error when a feature’s values are randomly permuted while all other variables are held fixed. Features that produce larger drops in predictive accuracy are therefore more important to the model. Because this metric is tied directly to out-of-sample performance, it avoids many of the biases associated with impurity-based importance measures.

Several clear patterns emerge. **Rent burden** is the most important predictor, highlighting the close relationship between housing cost pressure and neighborhood income. **Borough** indicators also rank highly, reflecting persistent geographic and administrative differences across New York City that are not fully captured by complaint metrics alone.

Measures of complaint composition are particularly informative. The **share of HPD complaints** and **share of 311 complaints** rank above simple complaint counts, suggesting that how residents engage with housing service systems matters more than overall complaint volume. Consistent with this, HPD complaint rates carry more predictive weight than DOB-related measures, which are largely uninformative once other features are included.

Resolution outcomes contribute meaningfully as well. **Average resolution time** and **heat-related 311 complaints** exhibit non-trivial importance, indicating that both service responsiveness and the presence of essential service failures vary systematically with neighborhood income. By contrast, **DOB complaint share and rate** add little incremental information.

Overall, the importance rankings suggest that structural housing stress, geographic context, complaint composition, and service outcomes jointly encode neighborhood income. These results should be interpreted as predictive relationships rather than causal effects.


## 6. Additional Analysis

For additional analysis, I created a set of "Tier-1 features", which are defined as the small set of predictors with the highest permutation importance in the global random forest model. These include `rent_burden`, `borough`, `share_hpd`, `share_311`, `hpd_rates`, and `resolution_days_avg`.

I conducted further permutation tests on the trained Random Forest, permuting each Tier-1 feature in the test set and measuring the resulting drop in out-of-sample R^2. All Tier-1 features produce statistically significant declines in predictive performance after multiple-testing correction, confirming that these variables contribute meaningfully to the model rather than serving as noise.

```{r}
#| code-summary: "Permutation Test and Initial Visualization"

# Tier features chosen from importance_df calculation 
tier1_main <- c( "rent_burden", "borough", "share_hpd", "share_311", "hpd_rate", "resolution_days_avg" )

# Permutation test setup:
# Null hypothesis (H0): Permuting a feature does not reduce out-of-sample predictive performance (R^2).
# Alternative (H1): Permuting a feature reduces R^2, implying the feature contributes to prediction.

# Out-of-sample R^2 helper (SSE/SST form) computed on a given dataset
rf_r2 <- function(model, df, y_col = "log_income") {
  pred <- predict(model, data = df)$predictions
  y <- df[[y_col]]
  1 - sum((y - pred)^2) / sum((y - mean(y))^2)
}

# Baseline test-set performance (used as the reference for all permutation drops)
baseline_r2 <- rf_r2(rf_income, test_df)

perm_test_feature_df <- function(model, df, feat, B = 250, seed = 123) {
  set.seed(seed)

  base <- rf_r2(model, df)

  drops_df <- tibble(
    feat = feat,
    b = seq_len(B),
    r2_perm = purrr::map_dbl(seq_len(B), function(i) {
      df_perm <- df
      df_perm[[.env$feat]] <- sample(df_perm[[.env$feat]])
      rf_r2(model, df_perm)
    })
  ) %>%
    mutate(
      baseline_r2 = base,
      drop = baseline_r2 - r2_perm
    )
  # One-sided permutation p-value (smoothed to avoid p = 0)
  p_val <- (sum(drops_df$drop <= 0) + 1) / (B + 1)

  summary_df <- drops_df %>%
    summarize(
      feature = first(feat),
      baseline_r2 = first(baseline_r2),
      mean_drop = mean(drop),
      median_drop = median(drop),
      q05 = unname(quantile(drop, 0.05)),
      q95 = unname(quantile(drop, 0.95)),
      p_value = p_val,
      .groups = "drop"
    )

  list(summary = summary_df, drops = drops_df %>% rename(feature = feat))
}

# Run permutation tests for each Tier-1 feature and on test set
perm_out <- purrr::map(
  tier1_main,
  ~perm_test_feature_df(rf_income, test_df, .x, B = 250, seed = 123)
)

# Summary table
rf_perm_main <- purrr::map_dfr(perm_out, "summary") %>%
  mutate(p_adj = p.adjust(p_value, method = "BH")) %>%
  arrange(p_adj)

# Long-format drops (for visualization)
rf_perm_drops <- purrr::map_dfr(perm_out, "drops")

#Predictive contribution by feature comparison visualization
ggplot(rf_perm_drops, aes(x = drop)) +
  geom_histogram(
    bins = 25,
    fill = "#302B2B",
    alpha = 0.65
  ) +
  geom_vline(
    xintercept = 0,
    linewidth = 0.8,
    color = "gray20"
  ) +
  facet_wrap(
    ~ feature,
    scales = "free_y"
  ) +
  labs(
    title = "Predictive Contribution by Feature",
    subtitle = "Distributions of out-of-sample ΔR^2  after feature permutation",
    x = expression(Delta*R^2~"(baseline − permuted)"),
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(size = 11),
    axis.text.y = element_text(size = 11),
    axis.title.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    strip.text = element_text(
      size = 12,
      face = "bold"
    ),
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30"),
    plot.margin = margin(t = 15, r = 10, b = 10, l = 10)
  )
```

For the full test set, permutation distributions are centered above zero, indicating that shuffling Tier-1 features typically degrades predictive accuracy.

To assess whether housing complaint signals operate differently across the income distribution, I conducted further conditional permutation tests by stratifying neighborhoods into income terciles. Within each group, I permute individual Tier-1 features and compute the corresponding drop in out-of-sample R^2. Differences in the magnitude of these drops indicate that certain complaint features contribute more strongly to income prediction in some income strata than others, highlighting meaningful heterogeneity in the model’s reliance on complaint-based information.

```{r}
#| code-summary: "Predictive Contribution by Income Group Visualization"

# Stratify the test set into income terciles
test_df <- test_df %>%
  mutate(income_bin = ntile(median_income, 3))

# Repeat permutation test for each tercile
perm_by_bin <- purrr::map_dfr(
  unique(test_df$income_bin),
  function(b) {
    df_sub <- test_df %>% filter(income_bin == b)

    purrr::map_dfr(
      tier1_main,
      ~perm_test_feature_df(rf_income, df_sub, .x)$summary %>%
        mutate(income_bin = b)
    )
  }
)

#Contribution by income group visualization
ggplot(
  perm_by_bin,
  aes(
    x = reorder(feature, mean_drop),
    y = mean_drop,
    fill = factor(income_bin)
  )
) +
  geom_col(
    position = position_dodge(width = 0.75),
    width = 0.65
  ) +
  coord_flip() +
  scale_fill_manual(
    values = c("#302B2B", "gray55", "gray80"),
    labels = c("Low income", "Middle income", "High income")
  ) +
  labs(
    title = "Predictive Contribution by Income Group",
    subtitle = "Permutation-induced ΔR^2 by neighborhood income tercile",
    x = NULL,
    y = expression(Delta*R^2),
    fill = "Income tercile"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 11),
    axis.title.y = element_text(size = 11),
    axis.title.x = element_text(size = 11),
    legend.title = element_text(size = 11),
    legend.text = element_text(size = 10),
    legend.position = "right",
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(color = "gray30"),
    plot.margin = margin(t = 15, r = 10, b = 10, l = 10)
  )
```

Although Tier-1 features are globally important for predicting neighborhood income in the full sample, their predictive contribution is not uniform across income strata. Conditional permutation tests reveal that some complaint-based features contribute little additional information once neighborhoods are already grouped by income level, leading to near-zero or slightly negative ΔR^2 values in certain terciles. This pattern reflects redundancy and reduced within-group variation rather than a contradiction of global importance, and is consistent with heterogeneous, nonlinear relationships captured by the random forest.


## 7. Discussion and Connection to the Overall Question
This analysis demonstrates that NYC’s housing complaint systems are closely linked to neighborhood income. Complaint intensity, complaint composition, and resolution outcomes together encode enough information to predict tract-level income with moderate to high accuracy. This finding has important implications for the team’s overall question.

First, it suggests that income influences not only housing conditions, but also how residents interact with housing service systems and how those systems respond. Complaint data therefore reflect a mixture of physical conditions, reporting behavior, and administrative processes. Second, it cautions against interpreting raw complaint counts as direct measures of housing quality without accounting for socioeconomic context.

For the team’s main OQ, these results imply that income-based disparities in complaint patterns may arise even if underlying housing conditions were identical. Analyses focused on complaint distribution, type, or resolution should therefore consider mechanisms such as differential reporting capacity, access to information, or landlord responsiveness.
Several limitations remain. Spatial misclassification and missing geocodes may bias tract-level aggregates. Resolution time is an imperfect proxy for service effectiveness.

Finally, this analysis uses a single year of data. Extending the framework to a multi-year panel and incorporating complementary records such as Housing Court filings would further clarify the relationship between income, housing conditions, and service interaction.

Overall, my findings support the broader project conclusion that neighborhood income is a fundamental organizing dimension of NYC’s housing complaint system, shaping both where complaints arise and how they are processed.